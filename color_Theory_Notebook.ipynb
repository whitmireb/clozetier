{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a csv file to use for an AI model for color selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "def generate_random_color(min_val, max_val):\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(\n",
    "        random.randint(min_val[0], max_val[0]),\n",
    "        random.randint(min_val[1], max_val[1]),\n",
    "        random.randint(min_val[2], max_val[2])\n",
    "    )\n",
    "\n",
    "def create_dataset(num_samples_per_category):\n",
    "    # Dictionary with color ranges to be used for classification\n",
    "    categories = {\n",
    "        'Black': [(0, 0, 0), (50, 50, 50)],\n",
    "        'White': [(200, 200, 200), (255, 255, 255)],\n",
    "        'Dark-Gray': [(50, 50, 50), (100, 100, 100)],\n",
    "        'Gray': [(100, 100, 100), (150, 150, 150)],\n",
    "        'Light-Gray': [(150, 150, 150), (200, 200, 200)],\n",
    "        'Dark-Blue': [(0, 0, 100), (0, 0, 139)],\n",
    "        'Blue': [(0, 0, 200), (0, 0, 255)],\n",
    "        'Light-Blue': [(173, 216, 230), (191, 239, 255)],\n",
    "        'Dark-Brown': [(60, 30, 10), (90, 45, 15)],\n",
    "        'Brown': [(139, 69, 19), (160, 82, 45)],\n",
    "        'Cream': [(245, 245, 220), (255, 253, 208)],\n",
    "        'Dark-Red': [(139, 0, 0), (165, 42, 42)],\n",
    "        'Red': [(255, 0, 0), (255, 99, 71)],\n",
    "        'Light-Red': [(255, 160, 122), (255, 182, 193)],\n",
    "        'Pink': [(255, 182, 193), (255, 192, 203)],\n",
    "        'Purple': [(128, 0, 128), (160, 32, 240)],\n",
    "        'Dark-Green': [(0, 100, 0), (0, 128, 0)],\n",
    "        'Green': [(0, 128, 0), (34, 139, 34)],\n",
    "        'Light-Green': [(144, 238, 144), (152, 251, 152)],\n",
    "        'Yellow': [(255, 255, 0), (255, 255, 102)],\n",
    "        'Orange': [(255, 140, 0), (255, 165, 0)],\n",
    "        'Peach': [(255, 218, 185), (255, 229, 180)],\n",
    "        'Gold': [(255, 215, 0), (255, 223, 102)],\n",
    "    }\n",
    "\n",
    "    with open('color_dataset.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Hex Color', 'Category'])\n",
    "        \n",
    "        for category, (min_val, max_val) in categories.items():\n",
    "            # Ensure the RGB values for each color range are valid\n",
    "            min_val = [min(min_val[i], max_val[i]) for i in range(3)]\n",
    "            max_val = [max(min_val[i], max_val[i]) for i in range(3)]\n",
    "            \n",
    "            for _ in range(num_samples_per_category):\n",
    "                hex_color = generate_random_color(min_val, max_val)\n",
    "                writer.writerow([hex_color, category])\n",
    "\n",
    "#generate teh dataset, total number is (classes * number in create_dataset)\n",
    "create_dataset(200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the CSV file and convert the Hex colors to RGB that way we can get a testing and training split for an AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('color_dataset.csv')\n",
    "\n",
    "# Convert Hex Color to RGB\n",
    "def hex_to_rgb(hex_color):\n",
    "    \"\"\"Convert a hex color to an RGB tuple.\"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "# Apply the RGB conversion to the entire dataset\n",
    "data['RGB'] = data['Hex Color'].apply(hex_to_rgb)\n",
    "\n",
    "# Separate features (RGB values) and labels (color category)\n",
    "X = data['RGB'].apply(pd.Series)  # Split RGB into separate columns\n",
    "y = data['Category']\n",
    "\n",
    "# Label encode the color categories\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our Neural Network architecture and set up the hyperparameters to set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ColorClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        #takes the input size and the output hidden size features\n",
    "        super(ColorClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #take the input tensor and apply ReLU activation\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3  # RGB has 3 input features\n",
    "hidden_size = 128\n",
    "num_classes = len(label_encoder.classes_)\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ColorClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 0.1169\n",
      "Epoch [20/10000], Loss: 0.1155\n",
      "Epoch [30/10000], Loss: 0.1141\n",
      "Epoch [40/10000], Loss: 0.1128\n",
      "Epoch [50/10000], Loss: 0.1116\n",
      "Epoch [60/10000], Loss: 0.1104\n",
      "Epoch [70/10000], Loss: 0.1092\n",
      "Epoch [80/10000], Loss: 0.1080\n",
      "Epoch [90/10000], Loss: 0.1069\n",
      "Epoch [100/10000], Loss: 0.1057\n",
      "Epoch [110/10000], Loss: 0.1046\n",
      "Epoch [120/10000], Loss: 0.1035\n",
      "Epoch [130/10000], Loss: 0.1024\n",
      "Epoch [140/10000], Loss: 0.1012\n",
      "Epoch [150/10000], Loss: 0.1002\n",
      "Epoch [160/10000], Loss: 0.0991\n",
      "Epoch [170/10000], Loss: 0.0980\n",
      "Epoch [180/10000], Loss: 0.0970\n",
      "Epoch [190/10000], Loss: 0.0959\n",
      "Epoch [200/10000], Loss: 0.0949\n",
      "Epoch [210/10000], Loss: 0.0938\n",
      "Epoch [220/10000], Loss: 0.0928\n",
      "Epoch [230/10000], Loss: 0.0918\n",
      "Epoch [240/10000], Loss: 0.0908\n",
      "Epoch [250/10000], Loss: 0.0898\n",
      "Epoch [260/10000], Loss: 0.0888\n",
      "Epoch [270/10000], Loss: 0.0878\n",
      "Epoch [280/10000], Loss: 0.0868\n",
      "Epoch [290/10000], Loss: 0.0859\n",
      "Epoch [300/10000], Loss: 0.0849\n",
      "Epoch [310/10000], Loss: 0.0840\n",
      "Epoch [320/10000], Loss: 0.0830\n",
      "Epoch [330/10000], Loss: 0.0821\n",
      "Epoch [340/10000], Loss: 0.0812\n",
      "Epoch [350/10000], Loss: 0.0803\n",
      "Epoch [360/10000], Loss: 0.0794\n",
      "Epoch [370/10000], Loss: 0.0785\n",
      "Epoch [380/10000], Loss: 0.0776\n",
      "Epoch [390/10000], Loss: 0.0768\n",
      "Epoch [400/10000], Loss: 0.0761\n",
      "Epoch [410/10000], Loss: 0.0762\n",
      "Epoch [420/10000], Loss: 0.0770\n",
      "Epoch [430/10000], Loss: 0.0740\n",
      "Epoch [440/10000], Loss: 0.0757\n",
      "Epoch [450/10000], Loss: 0.0731\n",
      "Epoch [460/10000], Loss: 0.0733\n",
      "Epoch [470/10000], Loss: 0.0723\n",
      "Epoch [480/10000], Loss: 0.0711\n",
      "Epoch [490/10000], Loss: 0.0723\n",
      "Epoch [500/10000], Loss: 0.0703\n",
      "Epoch [510/10000], Loss: 0.0690\n",
      "Epoch [520/10000], Loss: 0.0668\n",
      "Epoch [530/10000], Loss: 0.0763\n",
      "Epoch [540/10000], Loss: 0.0646\n",
      "Epoch [550/10000], Loss: 0.0684\n",
      "Epoch [560/10000], Loss: 0.0644\n",
      "Epoch [570/10000], Loss: 0.0638\n",
      "Epoch [580/10000], Loss: 0.0683\n",
      "Epoch [590/10000], Loss: 0.0609\n",
      "Epoch [600/10000], Loss: 0.0609\n",
      "Epoch [610/10000], Loss: 0.0676\n",
      "Epoch [620/10000], Loss: 0.0592\n",
      "Epoch [630/10000], Loss: 0.0662\n",
      "Epoch [640/10000], Loss: 0.0576\n",
      "Epoch [650/10000], Loss: 0.0584\n",
      "Epoch [660/10000], Loss: 0.0618\n",
      "Epoch [670/10000], Loss: 0.0598\n",
      "Epoch [680/10000], Loss: 0.0567\n",
      "Epoch [690/10000], Loss: 0.0547\n",
      "Epoch [700/10000], Loss: 0.0589\n",
      "Epoch [710/10000], Loss: 0.0536\n",
      "Epoch [720/10000], Loss: 0.0574\n",
      "Epoch [730/10000], Loss: 0.0537\n",
      "Epoch [740/10000], Loss: 0.0518\n",
      "Epoch [750/10000], Loss: 0.0523\n",
      "Epoch [760/10000], Loss: 0.0648\n",
      "Epoch [770/10000], Loss: 0.0527\n",
      "Epoch [780/10000], Loss: 0.0504\n",
      "Epoch [790/10000], Loss: 0.0517\n",
      "Epoch [800/10000], Loss: 0.0488\n",
      "Epoch [810/10000], Loss: 0.0497\n",
      "Epoch [820/10000], Loss: 0.0495\n",
      "Epoch [830/10000], Loss: 0.0471\n",
      "Epoch [840/10000], Loss: 0.0467\n",
      "Epoch [850/10000], Loss: 0.0509\n",
      "Epoch [860/10000], Loss: 0.0491\n",
      "Epoch [870/10000], Loss: 0.0498\n",
      "Epoch [880/10000], Loss: 0.0451\n",
      "Epoch [890/10000], Loss: 0.0445\n",
      "Epoch [900/10000], Loss: 0.0444\n",
      "Epoch [910/10000], Loss: 0.0437\n",
      "Epoch [920/10000], Loss: 0.0432\n",
      "Epoch [930/10000], Loss: 0.0450\n",
      "Epoch [940/10000], Loss: 0.0528\n",
      "Epoch [950/10000], Loss: 0.0417\n",
      "Epoch [960/10000], Loss: 0.0418\n",
      "Epoch [970/10000], Loss: 0.0423\n",
      "Epoch [980/10000], Loss: 0.0404\n",
      "Epoch [990/10000], Loss: 0.0402\n",
      "Epoch [1000/10000], Loss: 0.0400\n",
      "Epoch [1010/10000], Loss: 0.0393\n",
      "Epoch [1020/10000], Loss: 0.0386\n",
      "Epoch [1030/10000], Loss: 0.0382\n",
      "Epoch [1040/10000], Loss: 0.0393\n",
      "Epoch [1050/10000], Loss: 0.1349\n",
      "Epoch [1060/10000], Loss: 0.0403\n",
      "Epoch [1070/10000], Loss: 0.0388\n",
      "Epoch [1080/10000], Loss: 0.0405\n",
      "Epoch [1090/10000], Loss: 0.0528\n",
      "Epoch [1100/10000], Loss: 0.0522\n",
      "Epoch [1110/10000], Loss: 0.0362\n",
      "Epoch [1120/10000], Loss: 0.0367\n",
      "Epoch [1130/10000], Loss: 0.0344\n",
      "Epoch [1140/10000], Loss: 0.0332\n",
      "Epoch [1150/10000], Loss: 0.0329\n",
      "Epoch [1160/10000], Loss: 0.0325\n",
      "Epoch [1170/10000], Loss: 0.0322\n",
      "Epoch [1180/10000], Loss: 0.0318\n",
      "Epoch [1190/10000], Loss: 0.0315\n",
      "Epoch [1200/10000], Loss: 0.0312\n",
      "Epoch [1210/10000], Loss: 0.0309\n",
      "Epoch [1220/10000], Loss: 0.0306\n",
      "Epoch [1230/10000], Loss: 0.0303\n",
      "Epoch [1240/10000], Loss: 0.0300\n",
      "Epoch [1250/10000], Loss: 0.0297\n",
      "Epoch [1260/10000], Loss: 0.0294\n",
      "Epoch [1270/10000], Loss: 0.0292\n",
      "Epoch [1280/10000], Loss: 0.0289\n",
      "Epoch [1290/10000], Loss: 0.0286\n",
      "Epoch [1300/10000], Loss: 0.0283\n",
      "Epoch [1310/10000], Loss: 0.0280\n",
      "Epoch [1320/10000], Loss: 0.0277\n",
      "Epoch [1330/10000], Loss: 0.0275\n",
      "Epoch [1340/10000], Loss: 0.0272\n",
      "Epoch [1350/10000], Loss: 0.0270\n",
      "Epoch [1360/10000], Loss: 0.0267\n",
      "Epoch [1370/10000], Loss: 0.0264\n",
      "Epoch [1380/10000], Loss: 0.0262\n",
      "Epoch [1390/10000], Loss: 0.0260\n",
      "Epoch [1400/10000], Loss: 0.0257\n",
      "Epoch [1410/10000], Loss: 0.0255\n",
      "Epoch [1420/10000], Loss: 0.0252\n",
      "Epoch [1430/10000], Loss: 0.0250\n",
      "Epoch [1440/10000], Loss: 0.0247\n",
      "Epoch [1450/10000], Loss: 0.0245\n",
      "Epoch [1460/10000], Loss: 0.0243\n",
      "Epoch [1470/10000], Loss: 0.0240\n",
      "Epoch [1480/10000], Loss: 0.0238\n",
      "Epoch [1490/10000], Loss: 0.0236\n",
      "Epoch [1500/10000], Loss: 0.0234\n",
      "Epoch [1510/10000], Loss: 0.0232\n",
      "Epoch [1520/10000], Loss: 0.0229\n",
      "Epoch [1530/10000], Loss: 0.0227\n",
      "Epoch [1540/10000], Loss: 0.0225\n",
      "Epoch [1550/10000], Loss: 0.0223\n",
      "Epoch [1560/10000], Loss: 0.0221\n",
      "Epoch [1570/10000], Loss: 0.0219\n",
      "Epoch [1580/10000], Loss: 0.0217\n",
      "Epoch [1590/10000], Loss: 0.0215\n",
      "Epoch [1600/10000], Loss: 0.0213\n",
      "Epoch [1610/10000], Loss: 0.0211\n",
      "Epoch [1620/10000], Loss: 0.0209\n",
      "Epoch [1630/10000], Loss: 0.0207\n",
      "Epoch [1640/10000], Loss: 0.0205\n",
      "Epoch [1650/10000], Loss: 0.0203\n",
      "Epoch [1660/10000], Loss: 0.0202\n",
      "Epoch [1670/10000], Loss: 0.0199\n",
      "Epoch [1680/10000], Loss: 0.0197\n",
      "Epoch [1690/10000], Loss: 0.0196\n",
      "Epoch [1700/10000], Loss: 0.0194\n",
      "Epoch [1710/10000], Loss: 0.0192\n",
      "Epoch [1720/10000], Loss: 0.0192\n",
      "Epoch [1730/10000], Loss: 0.0189\n",
      "Epoch [1740/10000], Loss: 0.0190\n",
      "Epoch [1750/10000], Loss: 0.0185\n",
      "Epoch [1760/10000], Loss: 0.0185\n",
      "Epoch [1770/10000], Loss: 0.0183\n",
      "Epoch [1780/10000], Loss: 0.0182\n",
      "Epoch [1790/10000], Loss: 0.0184\n",
      "Epoch [1800/10000], Loss: 0.2308\n",
      "Epoch [1810/10000], Loss: 2.1448\n",
      "Epoch [1820/10000], Loss: 0.8836\n",
      "Epoch [1830/10000], Loss: 0.3570\n",
      "Epoch [1840/10000], Loss: 0.1334\n",
      "Epoch [1850/10000], Loss: 0.1008\n",
      "Epoch [1860/10000], Loss: 0.0718\n",
      "Epoch [1870/10000], Loss: 0.0584\n",
      "Epoch [1880/10000], Loss: 0.0545\n",
      "Epoch [1890/10000], Loss: 0.0526\n",
      "Epoch [1900/10000], Loss: 0.0512\n",
      "Epoch [1910/10000], Loss: 0.0503\n",
      "Epoch [1920/10000], Loss: 0.0496\n",
      "Epoch [1930/10000], Loss: 0.0489\n",
      "Epoch [1940/10000], Loss: 0.0483\n",
      "Epoch [1950/10000], Loss: 0.0477\n",
      "Epoch [1960/10000], Loss: 0.0472\n",
      "Epoch [1970/10000], Loss: 0.0467\n",
      "Epoch [1980/10000], Loss: 0.0463\n",
      "Epoch [1990/10000], Loss: 0.0457\n",
      "Epoch [2000/10000], Loss: 0.0453\n",
      "Epoch [2010/10000], Loss: 0.0448\n",
      "Epoch [2020/10000], Loss: 0.0443\n",
      "Epoch [2030/10000], Loss: 0.0439\n",
      "Epoch [2040/10000], Loss: 0.0434\n",
      "Epoch [2050/10000], Loss: 0.0430\n",
      "Epoch [2060/10000], Loss: 0.0426\n",
      "Epoch [2070/10000], Loss: 0.0422\n",
      "Epoch [2080/10000], Loss: 0.0419\n",
      "Epoch [2090/10000], Loss: 0.0415\n",
      "Epoch [2100/10000], Loss: 0.0412\n",
      "Epoch [2110/10000], Loss: 0.0407\n",
      "Epoch [2120/10000], Loss: 0.0401\n",
      "Epoch [2130/10000], Loss: 0.0392\n",
      "Epoch [2140/10000], Loss: 0.0381\n",
      "Epoch [2150/10000], Loss: 0.0333\n",
      "Epoch [2160/10000], Loss: 0.0309\n",
      "Epoch [2170/10000], Loss: 0.0301\n",
      "Epoch [2180/10000], Loss: 0.0296\n",
      "Epoch [2190/10000], Loss: 0.0291\n",
      "Epoch [2200/10000], Loss: 0.0288\n",
      "Epoch [2210/10000], Loss: 0.0285\n",
      "Epoch [2220/10000], Loss: 0.0282\n",
      "Epoch [2230/10000], Loss: 0.0279\n",
      "Epoch [2240/10000], Loss: 0.0277\n",
      "Epoch [2250/10000], Loss: 0.0275\n",
      "Epoch [2260/10000], Loss: 0.0273\n",
      "Epoch [2270/10000], Loss: 0.0271\n",
      "Epoch [2280/10000], Loss: 0.0269\n",
      "Epoch [2290/10000], Loss: 0.0267\n",
      "Epoch [2300/10000], Loss: 0.0265\n",
      "Epoch [2310/10000], Loss: 0.0264\n",
      "Epoch [2320/10000], Loss: 0.0262\n",
      "Epoch [2330/10000], Loss: 0.0260\n",
      "Epoch [2340/10000], Loss: 0.0258\n",
      "Epoch [2350/10000], Loss: 0.0256\n",
      "Epoch [2360/10000], Loss: 0.0253\n",
      "Epoch [2370/10000], Loss: 0.0251\n",
      "Epoch [2380/10000], Loss: 0.0248\n",
      "Epoch [2390/10000], Loss: 0.0245\n",
      "Epoch [2400/10000], Loss: 0.0243\n",
      "Epoch [2410/10000], Loss: 0.0241\n",
      "Epoch [2420/10000], Loss: 0.0239\n",
      "Epoch [2430/10000], Loss: 0.0237\n",
      "Epoch [2440/10000], Loss: 0.0236\n",
      "Epoch [2450/10000], Loss: 0.0234\n",
      "Epoch [2460/10000], Loss: 0.0233\n",
      "Epoch [2470/10000], Loss: 0.0231\n",
      "Epoch [2480/10000], Loss: 0.0229\n",
      "Epoch [2490/10000], Loss: 0.0228\n",
      "Epoch [2500/10000], Loss: 0.0226\n",
      "Epoch [2510/10000], Loss: 0.0224\n",
      "Epoch [2520/10000], Loss: 0.0223\n",
      "Epoch [2530/10000], Loss: 0.0221\n",
      "Epoch [2540/10000], Loss: 0.0220\n",
      "Epoch [2550/10000], Loss: 0.0218\n",
      "Epoch [2560/10000], Loss: 0.0217\n",
      "Epoch [2570/10000], Loss: 0.0215\n",
      "Epoch [2580/10000], Loss: 0.0214\n",
      "Epoch [2590/10000], Loss: 0.0212\n",
      "Epoch [2600/10000], Loss: 0.0211\n",
      "Epoch [2610/10000], Loss: 0.0209\n",
      "Epoch [2620/10000], Loss: 0.0208\n",
      "Epoch [2630/10000], Loss: 0.0207\n",
      "Epoch [2640/10000], Loss: 0.0205\n",
      "Epoch [2650/10000], Loss: 0.0204\n",
      "Epoch [2660/10000], Loss: 0.0203\n",
      "Epoch [2670/10000], Loss: 0.0202\n",
      "Epoch [2680/10000], Loss: 0.0200\n",
      "Epoch [2690/10000], Loss: 0.0199\n",
      "Epoch [2700/10000], Loss: 0.0198\n",
      "Epoch [2710/10000], Loss: 0.0197\n",
      "Epoch [2720/10000], Loss: 0.0196\n",
      "Epoch [2730/10000], Loss: 0.0195\n",
      "Epoch [2740/10000], Loss: 0.0193\n",
      "Epoch [2750/10000], Loss: 0.0192\n",
      "Epoch [2760/10000], Loss: 0.0191\n",
      "Epoch [2770/10000], Loss: 0.0190\n",
      "Epoch [2780/10000], Loss: 0.0189\n",
      "Epoch [2790/10000], Loss: 0.0188\n",
      "Epoch [2800/10000], Loss: 0.0187\n",
      "Epoch [2810/10000], Loss: 0.0186\n",
      "Epoch [2820/10000], Loss: 0.0184\n",
      "Epoch [2830/10000], Loss: 0.0183\n",
      "Epoch [2840/10000], Loss: 0.0182\n",
      "Epoch [2850/10000], Loss: 0.0181\n",
      "Epoch [2860/10000], Loss: 0.0180\n",
      "Epoch [2870/10000], Loss: 0.0179\n",
      "Epoch [2880/10000], Loss: 0.0178\n",
      "Epoch [2890/10000], Loss: 0.0177\n",
      "Epoch [2900/10000], Loss: 0.0176\n",
      "Epoch [2910/10000], Loss: 0.0175\n",
      "Epoch [2920/10000], Loss: 0.0174\n",
      "Epoch [2930/10000], Loss: 0.0172\n",
      "Epoch [2940/10000], Loss: 0.0171\n",
      "Epoch [2950/10000], Loss: 0.0170\n",
      "Epoch [2960/10000], Loss: 0.0169\n",
      "Epoch [2970/10000], Loss: 0.0168\n",
      "Epoch [2980/10000], Loss: 0.0167\n",
      "Epoch [2990/10000], Loss: 0.0166\n",
      "Epoch [3000/10000], Loss: 0.0165\n",
      "Epoch [3010/10000], Loss: 0.0164\n",
      "Epoch [3020/10000], Loss: 0.0163\n",
      "Epoch [3030/10000], Loss: 0.0162\n",
      "Epoch [3040/10000], Loss: 0.0161\n",
      "Epoch [3050/10000], Loss: 0.0160\n",
      "Epoch [3060/10000], Loss: 0.0159\n",
      "Epoch [3070/10000], Loss: 0.0158\n",
      "Epoch [3080/10000], Loss: 0.0157\n",
      "Epoch [3090/10000], Loss: 0.0157\n",
      "Epoch [3100/10000], Loss: 0.0156\n",
      "Epoch [3110/10000], Loss: 0.0155\n",
      "Epoch [3120/10000], Loss: 0.0154\n",
      "Epoch [3130/10000], Loss: 0.0153\n",
      "Epoch [3140/10000], Loss: 0.0152\n",
      "Epoch [3150/10000], Loss: 0.0151\n",
      "Epoch [3160/10000], Loss: 0.0151\n",
      "Epoch [3170/10000], Loss: 0.0150\n",
      "Epoch [3180/10000], Loss: 0.0149\n",
      "Epoch [3190/10000], Loss: 0.0148\n",
      "Epoch [3200/10000], Loss: 0.0147\n",
      "Epoch [3210/10000], Loss: 0.0147\n",
      "Epoch [3220/10000], Loss: 0.0146\n",
      "Epoch [3230/10000], Loss: 0.0145\n",
      "Epoch [3240/10000], Loss: 0.0144\n",
      "Epoch [3250/10000], Loss: 0.0143\n",
      "Epoch [3260/10000], Loss: 0.0143\n",
      "Epoch [3270/10000], Loss: 0.0142\n",
      "Epoch [3280/10000], Loss: 0.0141\n",
      "Epoch [3290/10000], Loss: 0.0140\n",
      "Epoch [3300/10000], Loss: 0.0140\n",
      "Epoch [3310/10000], Loss: 0.0139\n",
      "Epoch [3320/10000], Loss: 0.0138\n",
      "Epoch [3330/10000], Loss: 0.0137\n",
      "Epoch [3340/10000], Loss: 0.0137\n",
      "Epoch [3350/10000], Loss: 0.0136\n",
      "Epoch [3360/10000], Loss: 0.0135\n",
      "Epoch [3370/10000], Loss: 0.0134\n",
      "Epoch [3380/10000], Loss: 0.0134\n",
      "Epoch [3390/10000], Loss: 0.0133\n",
      "Epoch [3400/10000], Loss: 0.0132\n",
      "Epoch [3410/10000], Loss: 0.0132\n",
      "Epoch [3420/10000], Loss: 0.0131\n",
      "Epoch [3430/10000], Loss: 0.0130\n",
      "Epoch [3440/10000], Loss: 0.0129\n",
      "Epoch [3450/10000], Loss: 0.0129\n",
      "Epoch [3460/10000], Loss: 0.0128\n",
      "Epoch [3470/10000], Loss: 0.0127\n",
      "Epoch [3480/10000], Loss: 0.0127\n",
      "Epoch [3490/10000], Loss: 0.0126\n",
      "Epoch [3500/10000], Loss: 0.0125\n",
      "Epoch [3510/10000], Loss: 0.0125\n",
      "Epoch [3520/10000], Loss: 0.0124\n",
      "Epoch [3530/10000], Loss: 0.0123\n",
      "Epoch [3540/10000], Loss: 0.0123\n",
      "Epoch [3550/10000], Loss: 0.0122\n",
      "Epoch [3560/10000], Loss: 0.0121\n",
      "Epoch [3570/10000], Loss: 0.0121\n",
      "Epoch [3580/10000], Loss: 0.0120\n",
      "Epoch [3590/10000], Loss: 0.0120\n",
      "Epoch [3600/10000], Loss: 0.0119\n",
      "Epoch [3610/10000], Loss: 0.0118\n",
      "Epoch [3620/10000], Loss: 0.0118\n",
      "Epoch [3630/10000], Loss: 0.0117\n",
      "Epoch [3640/10000], Loss: 0.0116\n",
      "Epoch [3650/10000], Loss: 0.0116\n",
      "Epoch [3660/10000], Loss: 0.0115\n",
      "Epoch [3670/10000], Loss: 0.0115\n",
      "Epoch [3680/10000], Loss: 0.0114\n",
      "Epoch [3690/10000], Loss: 0.0113\n",
      "Epoch [3700/10000], Loss: 0.0113\n",
      "Epoch [3710/10000], Loss: 0.0112\n",
      "Epoch [3720/10000], Loss: 0.0112\n",
      "Epoch [3730/10000], Loss: 0.0111\n",
      "Epoch [3740/10000], Loss: 0.0111\n",
      "Epoch [3750/10000], Loss: 0.0110\n",
      "Epoch [3760/10000], Loss: 0.0109\n",
      "Epoch [3770/10000], Loss: 0.0109\n",
      "Epoch [3780/10000], Loss: 0.0108\n",
      "Epoch [3790/10000], Loss: 0.0108\n",
      "Epoch [3800/10000], Loss: 0.0107\n",
      "Epoch [3810/10000], Loss: 0.0107\n",
      "Epoch [3820/10000], Loss: 0.0106\n",
      "Epoch [3830/10000], Loss: 0.0105\n",
      "Epoch [3840/10000], Loss: 0.0105\n",
      "Epoch [3850/10000], Loss: 0.0104\n",
      "Epoch [3860/10000], Loss: 0.0104\n",
      "Epoch [3870/10000], Loss: 0.0103\n",
      "Epoch [3880/10000], Loss: 0.0103\n",
      "Epoch [3890/10000], Loss: 0.0102\n",
      "Epoch [3900/10000], Loss: 0.0102\n",
      "Epoch [3910/10000], Loss: 0.0101\n",
      "Epoch [3920/10000], Loss: 0.0101\n",
      "Epoch [3930/10000], Loss: 0.0100\n",
      "Epoch [3940/10000], Loss: 0.0100\n",
      "Epoch [3950/10000], Loss: 0.0099\n",
      "Epoch [3960/10000], Loss: 0.0099\n",
      "Epoch [3970/10000], Loss: 0.0098\n",
      "Epoch [3980/10000], Loss: 0.0098\n",
      "Epoch [3990/10000], Loss: 0.0097\n",
      "Epoch [4000/10000], Loss: 0.0096\n",
      "Epoch [4010/10000], Loss: 0.0096\n",
      "Epoch [4020/10000], Loss: 0.0096\n",
      "Epoch [4030/10000], Loss: 0.0095\n",
      "Epoch [4040/10000], Loss: 0.0095\n",
      "Epoch [4050/10000], Loss: 0.0094\n",
      "Epoch [4060/10000], Loss: 0.0093\n",
      "Epoch [4070/10000], Loss: 0.0093\n",
      "Epoch [4080/10000], Loss: 0.0092\n",
      "Epoch [4090/10000], Loss: 0.0092\n",
      "Epoch [4100/10000], Loss: 0.0092\n",
      "Epoch [4110/10000], Loss: 0.0091\n",
      "Epoch [4120/10000], Loss: 0.0091\n",
      "Epoch [4130/10000], Loss: 0.0090\n",
      "Epoch [4140/10000], Loss: 0.0090\n",
      "Epoch [4150/10000], Loss: 0.0089\n",
      "Epoch [4160/10000], Loss: 0.0089\n",
      "Epoch [4170/10000], Loss: 0.0088\n",
      "Epoch [4180/10000], Loss: 0.0088\n",
      "Epoch [4190/10000], Loss: 0.0087\n",
      "Epoch [4200/10000], Loss: 0.0087\n",
      "Epoch [4210/10000], Loss: 0.0086\n",
      "Epoch [4220/10000], Loss: 0.0086\n",
      "Epoch [4230/10000], Loss: 0.0085\n",
      "Epoch [4240/10000], Loss: 0.0085\n",
      "Epoch [4250/10000], Loss: 0.0084\n",
      "Epoch [4260/10000], Loss: 0.0084\n",
      "Epoch [4270/10000], Loss: 0.0084\n",
      "Epoch [4280/10000], Loss: 0.0083\n",
      "Epoch [4290/10000], Loss: 0.0082\n",
      "Epoch [4300/10000], Loss: 0.0082\n",
      "Epoch [4310/10000], Loss: 0.0081\n",
      "Epoch [4320/10000], Loss: 0.0081\n",
      "Epoch [4330/10000], Loss: 0.0080\n",
      "Epoch [4340/10000], Loss: 0.0080\n",
      "Epoch [4350/10000], Loss: 0.0080\n",
      "Epoch [4360/10000], Loss: 0.0079\n",
      "Epoch [4370/10000], Loss: 0.0079\n",
      "Epoch [4380/10000], Loss: 0.0078\n",
      "Epoch [4390/10000], Loss: 0.0078\n",
      "Epoch [4400/10000], Loss: 0.0077\n",
      "Epoch [4410/10000], Loss: 0.0077\n",
      "Epoch [4420/10000], Loss: 0.0076\n",
      "Epoch [4430/10000], Loss: 0.0076\n",
      "Epoch [4440/10000], Loss: 0.0076\n",
      "Epoch [4450/10000], Loss: 0.0075\n",
      "Epoch [4460/10000], Loss: 0.0075\n",
      "Epoch [4470/10000], Loss: 0.0074\n",
      "Epoch [4480/10000], Loss: 0.0074\n",
      "Epoch [4490/10000], Loss: 0.0074\n",
      "Epoch [4500/10000], Loss: 0.0073\n",
      "Epoch [4510/10000], Loss: 0.0073\n",
      "Epoch [4520/10000], Loss: 0.0072\n",
      "Epoch [4530/10000], Loss: 0.0072\n",
      "Epoch [4540/10000], Loss: 0.0072\n",
      "Epoch [4550/10000], Loss: 0.0071\n",
      "Epoch [4560/10000], Loss: 0.0071\n",
      "Epoch [4570/10000], Loss: 0.0070\n",
      "Epoch [4580/10000], Loss: 0.0070\n",
      "Epoch [4590/10000], Loss: 0.0070\n",
      "Epoch [4600/10000], Loss: 0.0069\n",
      "Epoch [4610/10000], Loss: 0.0069\n",
      "Epoch [4620/10000], Loss: 0.0069\n",
      "Epoch [4630/10000], Loss: 0.0068\n",
      "Epoch [4640/10000], Loss: 0.0068\n",
      "Epoch [4650/10000], Loss: 0.0067\n",
      "Epoch [4660/10000], Loss: 0.0067\n",
      "Epoch [4670/10000], Loss: 0.0066\n",
      "Epoch [4680/10000], Loss: 0.0066\n",
      "Epoch [4690/10000], Loss: 0.0065\n",
      "Epoch [4700/10000], Loss: 0.0065\n",
      "Epoch [4710/10000], Loss: 0.0064\n",
      "Epoch [4720/10000], Loss: 0.0063\n",
      "Epoch [4730/10000], Loss: 0.0063\n",
      "Epoch [4740/10000], Loss: 0.0062\n",
      "Epoch [4750/10000], Loss: 0.0062\n",
      "Epoch [4760/10000], Loss: 0.0061\n",
      "Epoch [4770/10000], Loss: 0.0061\n",
      "Epoch [4780/10000], Loss: 0.0060\n",
      "Epoch [4790/10000], Loss: 0.0060\n",
      "Epoch [4800/10000], Loss: 0.0060\n",
      "Epoch [4810/10000], Loss: 0.0059\n",
      "Epoch [4820/10000], Loss: 0.0059\n",
      "Epoch [4830/10000], Loss: 0.0059\n",
      "Epoch [4840/10000], Loss: 0.0058\n",
      "Epoch [4850/10000], Loss: 0.0058\n",
      "Epoch [4860/10000], Loss: 0.0058\n",
      "Epoch [4870/10000], Loss: 0.0057\n",
      "Epoch [4880/10000], Loss: 0.0057\n",
      "Epoch [4890/10000], Loss: 0.0057\n",
      "Epoch [4900/10000], Loss: 0.0056\n",
      "Epoch [4910/10000], Loss: 0.0056\n",
      "Epoch [4920/10000], Loss: 0.0056\n",
      "Epoch [4930/10000], Loss: 0.0055\n",
      "Epoch [4940/10000], Loss: 0.0055\n",
      "Epoch [4950/10000], Loss: 0.0055\n",
      "Epoch [4960/10000], Loss: 0.0054\n",
      "Epoch [4970/10000], Loss: 0.0054\n",
      "Epoch [4980/10000], Loss: 0.0054\n",
      "Epoch [4990/10000], Loss: 0.0054\n",
      "Epoch [5000/10000], Loss: 0.0053\n",
      "Epoch [5010/10000], Loss: 0.0053\n",
      "Epoch [5020/10000], Loss: 0.0053\n",
      "Epoch [5030/10000], Loss: 0.0053\n",
      "Epoch [5040/10000], Loss: 0.0052\n",
      "Epoch [5050/10000], Loss: 0.0052\n",
      "Epoch [5060/10000], Loss: 0.0051\n",
      "Epoch [5070/10000], Loss: 0.0051\n",
      "Epoch [5080/10000], Loss: 0.0049\n",
      "Epoch [5090/10000], Loss: 0.0049\n",
      "Epoch [5100/10000], Loss: 0.0048\n",
      "Epoch [5110/10000], Loss: 0.0048\n",
      "Epoch [5120/10000], Loss: 0.0048\n",
      "Epoch [5130/10000], Loss: 0.0047\n",
      "Epoch [5140/10000], Loss: 0.0047\n",
      "Epoch [5150/10000], Loss: 0.0047\n",
      "Epoch [5160/10000], Loss: 0.0047\n",
      "Epoch [5170/10000], Loss: 0.0046\n",
      "Epoch [5180/10000], Loss: 0.0046\n",
      "Epoch [5190/10000], Loss: 0.0046\n",
      "Epoch [5200/10000], Loss: 0.0046\n",
      "Epoch [5210/10000], Loss: 0.0046\n",
      "Epoch [5220/10000], Loss: 0.0045\n",
      "Epoch [5230/10000], Loss: 0.0045\n",
      "Epoch [5240/10000], Loss: 0.0045\n",
      "Epoch [5250/10000], Loss: 0.0045\n",
      "Epoch [5260/10000], Loss: 0.0045\n",
      "Epoch [5270/10000], Loss: 0.0044\n",
      "Epoch [5280/10000], Loss: 0.0044\n",
      "Epoch [5290/10000], Loss: 0.0044\n",
      "Epoch [5300/10000], Loss: 0.0044\n",
      "Epoch [5310/10000], Loss: 0.0043\n",
      "Epoch [5320/10000], Loss: 0.0043\n",
      "Epoch [5330/10000], Loss: 0.0043\n",
      "Epoch [5340/10000], Loss: 0.0043\n",
      "Epoch [5350/10000], Loss: 0.0043\n",
      "Epoch [5360/10000], Loss: 0.0042\n",
      "Epoch [5370/10000], Loss: 0.0042\n",
      "Epoch [5380/10000], Loss: 0.0042\n",
      "Epoch [5390/10000], Loss: 0.0042\n",
      "Epoch [5400/10000], Loss: 0.0042\n",
      "Epoch [5410/10000], Loss: 0.0041\n",
      "Epoch [5420/10000], Loss: 0.0041\n",
      "Epoch [5430/10000], Loss: 0.0041\n",
      "Epoch [5440/10000], Loss: 0.0041\n",
      "Epoch [5450/10000], Loss: 0.0041\n",
      "Epoch [5460/10000], Loss: 0.0041\n",
      "Epoch [5470/10000], Loss: 0.0040\n",
      "Epoch [5480/10000], Loss: 0.0040\n",
      "Epoch [5490/10000], Loss: 0.0040\n",
      "Epoch [5500/10000], Loss: 0.0040\n",
      "Epoch [5510/10000], Loss: 0.0040\n",
      "Epoch [5520/10000], Loss: 0.0039\n",
      "Epoch [5530/10000], Loss: 0.0039\n",
      "Epoch [5540/10000], Loss: 0.0039\n",
      "Epoch [5550/10000], Loss: 0.0039\n",
      "Epoch [5560/10000], Loss: 0.0039\n",
      "Epoch [5570/10000], Loss: 0.0038\n",
      "Epoch [5580/10000], Loss: 0.0038\n",
      "Epoch [5590/10000], Loss: 0.0038\n",
      "Epoch [5600/10000], Loss: 0.0038\n",
      "Epoch [5610/10000], Loss: 0.0038\n",
      "Epoch [5620/10000], Loss: 0.0037\n",
      "Epoch [5630/10000], Loss: 0.0037\n",
      "Epoch [5640/10000], Loss: 0.0037\n",
      "Epoch [5650/10000], Loss: 0.0037\n",
      "Epoch [5660/10000], Loss: 0.0037\n",
      "Epoch [5670/10000], Loss: 0.0036\n",
      "Epoch [5680/10000], Loss: 0.0036\n",
      "Epoch [5690/10000], Loss: 0.0036\n",
      "Epoch [5700/10000], Loss: 0.0036\n",
      "Epoch [5710/10000], Loss: 0.0036\n",
      "Epoch [5720/10000], Loss: 0.0036\n",
      "Epoch [5730/10000], Loss: 0.0036\n",
      "Epoch [5740/10000], Loss: 0.0035\n",
      "Epoch [5750/10000], Loss: 0.0035\n",
      "Epoch [5760/10000], Loss: 0.0035\n",
      "Epoch [5770/10000], Loss: 0.0035\n",
      "Epoch [5780/10000], Loss: 0.0034\n",
      "Epoch [5790/10000], Loss: 0.0034\n",
      "Epoch [5800/10000], Loss: 0.0034\n",
      "Epoch [5810/10000], Loss: 0.0034\n",
      "Epoch [5820/10000], Loss: 0.0034\n",
      "Epoch [5830/10000], Loss: 0.0034\n",
      "Epoch [5840/10000], Loss: 0.0033\n",
      "Epoch [5850/10000], Loss: 0.0033\n",
      "Epoch [5860/10000], Loss: 0.0033\n",
      "Epoch [5870/10000], Loss: 0.0033\n",
      "Epoch [5880/10000], Loss: 0.0033\n",
      "Epoch [5890/10000], Loss: 0.0033\n",
      "Epoch [5900/10000], Loss: 0.0032\n",
      "Epoch [5910/10000], Loss: 0.0032\n",
      "Epoch [5920/10000], Loss: 0.0032\n",
      "Epoch [5930/10000], Loss: 0.0032\n",
      "Epoch [5940/10000], Loss: 0.0032\n",
      "Epoch [5950/10000], Loss: 0.0032\n",
      "Epoch [5960/10000], Loss: 0.0031\n",
      "Epoch [5970/10000], Loss: 0.0031\n",
      "Epoch [5980/10000], Loss: 0.0031\n",
      "Epoch [5990/10000], Loss: 0.0031\n",
      "Epoch [6000/10000], Loss: 0.0031\n",
      "Epoch [6010/10000], Loss: 0.0031\n",
      "Epoch [6020/10000], Loss: 0.0031\n",
      "Epoch [6030/10000], Loss: 0.0030\n",
      "Epoch [6040/10000], Loss: 0.0030\n",
      "Epoch [6050/10000], Loss: 0.0030\n",
      "Epoch [6060/10000], Loss: 0.0030\n",
      "Epoch [6070/10000], Loss: 0.0030\n",
      "Epoch [6080/10000], Loss: 0.0030\n",
      "Epoch [6090/10000], Loss: 0.0031\n",
      "Epoch [6100/10000], Loss: 0.0032\n",
      "Epoch [6110/10000], Loss: 4.3503\n",
      "Epoch [6120/10000], Loss: 1.4376\n",
      "Epoch [6130/10000], Loss: 0.2496\n",
      "Epoch [6140/10000], Loss: 0.1106\n",
      "Epoch [6150/10000], Loss: 0.0561\n",
      "Epoch [6160/10000], Loss: 0.0388\n",
      "Epoch [6170/10000], Loss: 0.0346\n",
      "Epoch [6180/10000], Loss: 0.0312\n",
      "Epoch [6190/10000], Loss: 0.0296\n",
      "Epoch [6200/10000], Loss: 0.0287\n",
      "Epoch [6210/10000], Loss: 0.0276\n",
      "Epoch [6220/10000], Loss: 0.0267\n",
      "Epoch [6230/10000], Loss: 0.0258\n",
      "Epoch [6240/10000], Loss: 0.0251\n",
      "Epoch [6250/10000], Loss: 0.0246\n",
      "Epoch [6260/10000], Loss: 0.0241\n",
      "Epoch [6270/10000], Loss: 0.0237\n",
      "Epoch [6280/10000], Loss: 0.0234\n",
      "Epoch [6290/10000], Loss: 0.0231\n",
      "Epoch [6300/10000], Loss: 0.0228\n",
      "Epoch [6310/10000], Loss: 0.0226\n",
      "Epoch [6320/10000], Loss: 0.0224\n",
      "Epoch [6330/10000], Loss: 0.0222\n",
      "Epoch [6340/10000], Loss: 0.0221\n",
      "Epoch [6350/10000], Loss: 0.0219\n",
      "Epoch [6360/10000], Loss: 0.0217\n",
      "Epoch [6370/10000], Loss: 0.0215\n",
      "Epoch [6380/10000], Loss: 0.0213\n",
      "Epoch [6390/10000], Loss: 0.0212\n",
      "Epoch [6400/10000], Loss: 0.0210\n",
      "Epoch [6410/10000], Loss: 0.0209\n",
      "Epoch [6420/10000], Loss: 0.0207\n",
      "Epoch [6430/10000], Loss: 0.0206\n",
      "Epoch [6440/10000], Loss: 0.0204\n",
      "Epoch [6450/10000], Loss: 0.0203\n",
      "Epoch [6460/10000], Loss: 0.0200\n",
      "Epoch [6470/10000], Loss: 0.0198\n",
      "Epoch [6480/10000], Loss: 0.0191\n",
      "Epoch [6490/10000], Loss: 0.0189\n",
      "Epoch [6500/10000], Loss: 0.0187\n",
      "Epoch [6510/10000], Loss: 0.0185\n",
      "Epoch [6520/10000], Loss: 0.0183\n",
      "Epoch [6530/10000], Loss: 0.0182\n",
      "Epoch [6540/10000], Loss: 0.0181\n",
      "Epoch [6550/10000], Loss: 0.0179\n",
      "Epoch [6560/10000], Loss: 0.0178\n",
      "Epoch [6570/10000], Loss: 0.0177\n",
      "Epoch [6580/10000], Loss: 0.0176\n",
      "Epoch [6590/10000], Loss: 0.0175\n",
      "Epoch [6600/10000], Loss: 0.0174\n",
      "Epoch [6610/10000], Loss: 0.0173\n",
      "Epoch [6620/10000], Loss: 0.0172\n",
      "Epoch [6630/10000], Loss: 0.0170\n",
      "Epoch [6640/10000], Loss: 0.0169\n",
      "Epoch [6650/10000], Loss: 0.0168\n",
      "Epoch [6660/10000], Loss: 0.0168\n",
      "Epoch [6670/10000], Loss: 0.0167\n",
      "Epoch [6680/10000], Loss: 0.0166\n",
      "Epoch [6690/10000], Loss: 0.0165\n",
      "Epoch [6700/10000], Loss: 0.0163\n",
      "Epoch [6710/10000], Loss: 0.0162\n",
      "Epoch [6720/10000], Loss: 0.0161\n",
      "Epoch [6730/10000], Loss: 0.0160\n",
      "Epoch [6740/10000], Loss: 0.0158\n",
      "Epoch [6750/10000], Loss: 0.0157\n",
      "Epoch [6760/10000], Loss: 0.0156\n",
      "Epoch [6770/10000], Loss: 0.0155\n",
      "Epoch [6780/10000], Loss: 0.0154\n",
      "Epoch [6790/10000], Loss: 0.0153\n",
      "Epoch [6800/10000], Loss: 0.0151\n",
      "Epoch [6810/10000], Loss: 0.0150\n",
      "Epoch [6820/10000], Loss: 0.0149\n",
      "Epoch [6830/10000], Loss: 0.0148\n",
      "Epoch [6840/10000], Loss: 0.0147\n",
      "Epoch [6850/10000], Loss: 0.0146\n",
      "Epoch [6860/10000], Loss: 0.0144\n",
      "Epoch [6870/10000], Loss: 0.0143\n",
      "Epoch [6880/10000], Loss: 0.0142\n",
      "Epoch [6890/10000], Loss: 0.0141\n",
      "Epoch [6900/10000], Loss: 0.0140\n",
      "Epoch [6910/10000], Loss: 0.0140\n",
      "Epoch [6920/10000], Loss: 0.0139\n",
      "Epoch [6930/10000], Loss: 0.0138\n",
      "Epoch [6940/10000], Loss: 0.0137\n",
      "Epoch [6950/10000], Loss: 0.0137\n",
      "Epoch [6960/10000], Loss: 0.0136\n",
      "Epoch [6970/10000], Loss: 0.0135\n",
      "Epoch [6980/10000], Loss: 0.0135\n",
      "Epoch [6990/10000], Loss: 0.0134\n",
      "Epoch [7000/10000], Loss: 0.0133\n",
      "Epoch [7010/10000], Loss: 0.0133\n",
      "Epoch [7020/10000], Loss: 0.0132\n",
      "Epoch [7030/10000], Loss: 0.0131\n",
      "Epoch [7040/10000], Loss: 0.0131\n",
      "Epoch [7050/10000], Loss: 0.0130\n",
      "Epoch [7060/10000], Loss: 0.0129\n",
      "Epoch [7070/10000], Loss: 0.0129\n",
      "Epoch [7080/10000], Loss: 0.0128\n",
      "Epoch [7090/10000], Loss: 0.0128\n",
      "Epoch [7100/10000], Loss: 0.0127\n",
      "Epoch [7110/10000], Loss: 0.0127\n",
      "Epoch [7120/10000], Loss: 0.0126\n",
      "Epoch [7130/10000], Loss: 0.0126\n",
      "Epoch [7140/10000], Loss: 0.0125\n",
      "Epoch [7150/10000], Loss: 0.0125\n",
      "Epoch [7160/10000], Loss: 0.0124\n",
      "Epoch [7170/10000], Loss: 0.0124\n",
      "Epoch [7180/10000], Loss: 0.0124\n",
      "Epoch [7190/10000], Loss: 0.0123\n",
      "Epoch [7200/10000], Loss: 0.0123\n",
      "Epoch [7210/10000], Loss: 0.0122\n",
      "Epoch [7220/10000], Loss: 0.0122\n",
      "Epoch [7230/10000], Loss: 0.0121\n",
      "Epoch [7240/10000], Loss: 0.0121\n",
      "Epoch [7250/10000], Loss: 0.0120\n",
      "Epoch [7260/10000], Loss: 0.0120\n",
      "Epoch [7270/10000], Loss: 0.0119\n",
      "Epoch [7280/10000], Loss: 0.0119\n",
      "Epoch [7290/10000], Loss: 0.0118\n",
      "Epoch [7300/10000], Loss: 0.0118\n",
      "Epoch [7310/10000], Loss: 0.0118\n",
      "Epoch [7320/10000], Loss: 0.0117\n",
      "Epoch [7330/10000], Loss: 0.0117\n",
      "Epoch [7340/10000], Loss: 0.0116\n",
      "Epoch [7350/10000], Loss: 0.0116\n",
      "Epoch [7360/10000], Loss: 0.0115\n",
      "Epoch [7370/10000], Loss: 0.0114\n",
      "Epoch [7380/10000], Loss: 0.0114\n",
      "Epoch [7390/10000], Loss: 0.0113\n",
      "Epoch [7400/10000], Loss: 0.0112\n",
      "Epoch [7410/10000], Loss: 0.0112\n",
      "Epoch [7420/10000], Loss: 0.0111\n",
      "Epoch [7430/10000], Loss: 0.0110\n",
      "Epoch [7440/10000], Loss: 0.0110\n",
      "Epoch [7450/10000], Loss: 0.0109\n",
      "Epoch [7460/10000], Loss: 0.0108\n",
      "Epoch [7470/10000], Loss: 0.0108\n",
      "Epoch [7480/10000], Loss: 0.0107\n",
      "Epoch [7490/10000], Loss: 0.0107\n",
      "Epoch [7500/10000], Loss: 0.0106\n",
      "Epoch [7510/10000], Loss: 0.0106\n",
      "Epoch [7520/10000], Loss: 0.0106\n",
      "Epoch [7530/10000], Loss: 0.0105\n",
      "Epoch [7540/10000], Loss: 0.0105\n",
      "Epoch [7550/10000], Loss: 0.0104\n",
      "Epoch [7560/10000], Loss: 0.0104\n",
      "Epoch [7570/10000], Loss: 0.0104\n",
      "Epoch [7580/10000], Loss: 0.0103\n",
      "Epoch [7590/10000], Loss: 0.0103\n",
      "Epoch [7600/10000], Loss: 0.0103\n",
      "Epoch [7610/10000], Loss: 0.0102\n",
      "Epoch [7620/10000], Loss: 0.0102\n",
      "Epoch [7630/10000], Loss: 0.0101\n",
      "Epoch [7640/10000], Loss: 0.0101\n",
      "Epoch [7650/10000], Loss: 0.0101\n",
      "Epoch [7660/10000], Loss: 0.0100\n",
      "Epoch [7670/10000], Loss: 0.0100\n",
      "Epoch [7680/10000], Loss: 0.0100\n",
      "Epoch [7690/10000], Loss: 0.0099\n",
      "Epoch [7700/10000], Loss: 0.0099\n",
      "Epoch [7710/10000], Loss: 0.0099\n",
      "Epoch [7720/10000], Loss: 0.0098\n",
      "Epoch [7730/10000], Loss: 0.0098\n",
      "Epoch [7740/10000], Loss: 0.0098\n",
      "Epoch [7750/10000], Loss: 0.0098\n",
      "Epoch [7760/10000], Loss: 0.0097\n",
      "Epoch [7770/10000], Loss: 0.0097\n",
      "Epoch [7780/10000], Loss: 0.0097\n",
      "Epoch [7790/10000], Loss: 0.0096\n",
      "Epoch [7800/10000], Loss: 0.0096\n",
      "Epoch [7810/10000], Loss: 0.0096\n",
      "Epoch [7820/10000], Loss: 0.0095\n",
      "Epoch [7830/10000], Loss: 0.0095\n",
      "Epoch [7840/10000], Loss: 0.0095\n",
      "Epoch [7850/10000], Loss: 0.0094\n",
      "Epoch [7860/10000], Loss: 0.0094\n",
      "Epoch [7870/10000], Loss: 0.0094\n",
      "Epoch [7880/10000], Loss: 0.0094\n",
      "Epoch [7890/10000], Loss: 0.0093\n",
      "Epoch [7900/10000], Loss: 0.0093\n",
      "Epoch [7910/10000], Loss: 0.0093\n",
      "Epoch [7920/10000], Loss: 0.0092\n",
      "Epoch [7930/10000], Loss: 0.0092\n",
      "Epoch [7940/10000], Loss: 0.0092\n",
      "Epoch [7950/10000], Loss: 0.0092\n",
      "Epoch [7960/10000], Loss: 0.0091\n",
      "Epoch [7970/10000], Loss: 0.0091\n",
      "Epoch [7980/10000], Loss: 0.0091\n",
      "Epoch [7990/10000], Loss: 0.0090\n",
      "Epoch [8000/10000], Loss: 0.0090\n",
      "Epoch [8010/10000], Loss: 0.0090\n",
      "Epoch [8020/10000], Loss: 0.0089\n",
      "Epoch [8030/10000], Loss: 0.0089\n",
      "Epoch [8040/10000], Loss: 0.0089\n",
      "Epoch [8050/10000], Loss: 0.0089\n",
      "Epoch [8060/10000], Loss: 0.0088\n",
      "Epoch [8070/10000], Loss: 0.0088\n",
      "Epoch [8080/10000], Loss: 0.0088\n",
      "Epoch [8090/10000], Loss: 0.0088\n",
      "Epoch [8100/10000], Loss: 0.0087\n",
      "Epoch [8110/10000], Loss: 0.0087\n",
      "Epoch [8120/10000], Loss: 0.0087\n",
      "Epoch [8130/10000], Loss: 0.0086\n",
      "Epoch [8140/10000], Loss: 0.0086\n",
      "Epoch [8150/10000], Loss: 0.0086\n",
      "Epoch [8160/10000], Loss: 0.0086\n",
      "Epoch [8170/10000], Loss: 0.0085\n",
      "Epoch [8180/10000], Loss: 0.0085\n",
      "Epoch [8190/10000], Loss: 0.0085\n",
      "Epoch [8200/10000], Loss: 0.0085\n",
      "Epoch [8210/10000], Loss: 0.0084\n",
      "Epoch [8220/10000], Loss: 0.0084\n",
      "Epoch [8230/10000], Loss: 0.0084\n",
      "Epoch [8240/10000], Loss: 0.0084\n",
      "Epoch [8250/10000], Loss: 0.0083\n",
      "Epoch [8260/10000], Loss: 0.0083\n",
      "Epoch [8270/10000], Loss: 0.0083\n",
      "Epoch [8280/10000], Loss: 0.0083\n",
      "Epoch [8290/10000], Loss: 0.0082\n",
      "Epoch [8300/10000], Loss: 0.0082\n",
      "Epoch [8310/10000], Loss: 0.0082\n",
      "Epoch [8320/10000], Loss: 0.0081\n",
      "Epoch [8330/10000], Loss: 0.0081\n",
      "Epoch [8340/10000], Loss: 0.0080\n",
      "Epoch [8350/10000], Loss: 0.0080\n",
      "Epoch [8360/10000], Loss: 0.0080\n",
      "Epoch [8370/10000], Loss: 0.0079\n",
      "Epoch [8380/10000], Loss: 0.0079\n",
      "Epoch [8390/10000], Loss: 0.0078\n",
      "Epoch [8400/10000], Loss: 0.0078\n",
      "Epoch [8410/10000], Loss: 0.0078\n",
      "Epoch [8420/10000], Loss: 0.0078\n",
      "Epoch [8430/10000], Loss: 0.0077\n",
      "Epoch [8440/10000], Loss: 0.0077\n",
      "Epoch [8450/10000], Loss: 0.0077\n",
      "Epoch [8460/10000], Loss: 0.0077\n",
      "Epoch [8470/10000], Loss: 0.0076\n",
      "Epoch [8480/10000], Loss: 0.0076\n",
      "Epoch [8490/10000], Loss: 0.0076\n",
      "Epoch [8500/10000], Loss: 0.0076\n",
      "Epoch [8510/10000], Loss: 0.0075\n",
      "Epoch [8520/10000], Loss: 0.0075\n",
      "Epoch [8530/10000], Loss: 0.0075\n",
      "Epoch [8540/10000], Loss: 0.0075\n",
      "Epoch [8550/10000], Loss: 0.0075\n",
      "Epoch [8560/10000], Loss: 0.0074\n",
      "Epoch [8570/10000], Loss: 0.0074\n",
      "Epoch [8580/10000], Loss: 0.0074\n",
      "Epoch [8590/10000], Loss: 0.0074\n",
      "Epoch [8600/10000], Loss: 0.0073\n",
      "Epoch [8610/10000], Loss: 0.0073\n",
      "Epoch [8620/10000], Loss: 0.0073\n",
      "Epoch [8630/10000], Loss: 0.0073\n",
      "Epoch [8640/10000], Loss: 0.0073\n",
      "Epoch [8650/10000], Loss: 0.0072\n",
      "Epoch [8660/10000], Loss: 0.0072\n",
      "Epoch [8670/10000], Loss: 0.0072\n",
      "Epoch [8680/10000], Loss: 0.0072\n",
      "Epoch [8690/10000], Loss: 0.0071\n",
      "Epoch [8700/10000], Loss: 0.0071\n",
      "Epoch [8710/10000], Loss: 0.0071\n",
      "Epoch [8720/10000], Loss: 0.0071\n",
      "Epoch [8730/10000], Loss: 0.0071\n",
      "Epoch [8740/10000], Loss: 0.0070\n",
      "Epoch [8750/10000], Loss: 0.0070\n",
      "Epoch [8760/10000], Loss: 0.0070\n",
      "Epoch [8770/10000], Loss: 0.0070\n",
      "Epoch [8780/10000], Loss: 0.0070\n",
      "Epoch [8790/10000], Loss: 0.0069\n",
      "Epoch [8800/10000], Loss: 0.0069\n",
      "Epoch [8810/10000], Loss: 0.0069\n",
      "Epoch [8820/10000], Loss: 0.0069\n",
      "Epoch [8830/10000], Loss: 0.0068\n",
      "Epoch [8840/10000], Loss: 0.0068\n",
      "Epoch [8850/10000], Loss: 0.0068\n",
      "Epoch [8860/10000], Loss: 0.0068\n",
      "Epoch [8870/10000], Loss: 0.0068\n",
      "Epoch [8880/10000], Loss: 0.0067\n",
      "Epoch [8890/10000], Loss: 0.0067\n",
      "Epoch [8900/10000], Loss: 0.0067\n",
      "Epoch [8910/10000], Loss: 0.0067\n",
      "Epoch [8920/10000], Loss: 0.0067\n",
      "Epoch [8930/10000], Loss: 0.0066\n",
      "Epoch [8940/10000], Loss: 0.0066\n",
      "Epoch [8950/10000], Loss: 0.0066\n",
      "Epoch [8960/10000], Loss: 0.0066\n",
      "Epoch [8970/10000], Loss: 0.0066\n",
      "Epoch [8980/10000], Loss: 0.0065\n",
      "Epoch [8990/10000], Loss: 0.0065\n",
      "Epoch [9000/10000], Loss: 0.0065\n",
      "Epoch [9010/10000], Loss: 0.0065\n",
      "Epoch [9020/10000], Loss: 0.0064\n",
      "Epoch [9030/10000], Loss: 0.0064\n",
      "Epoch [9040/10000], Loss: 0.0064\n",
      "Epoch [9050/10000], Loss: 0.0064\n",
      "Epoch [9060/10000], Loss: 0.0064\n",
      "Epoch [9070/10000], Loss: 0.0063\n",
      "Epoch [9080/10000], Loss: 0.0063\n",
      "Epoch [9090/10000], Loss: 0.0063\n",
      "Epoch [9100/10000], Loss: 0.0063\n",
      "Epoch [9110/10000], Loss: 0.0063\n",
      "Epoch [9120/10000], Loss: 0.0062\n",
      "Epoch [9130/10000], Loss: 0.0062\n",
      "Epoch [9140/10000], Loss: 0.0062\n",
      "Epoch [9150/10000], Loss: 0.0062\n",
      "Epoch [9160/10000], Loss: 0.0062\n",
      "Epoch [9170/10000], Loss: 0.0061\n",
      "Epoch [9180/10000], Loss: 0.0061\n",
      "Epoch [9190/10000], Loss: 0.0061\n",
      "Epoch [9200/10000], Loss: 0.0061\n",
      "Epoch [9210/10000], Loss: 0.0061\n",
      "Epoch [9220/10000], Loss: 0.0060\n",
      "Epoch [9230/10000], Loss: 0.0060\n",
      "Epoch [9240/10000], Loss: 0.0060\n",
      "Epoch [9250/10000], Loss: 0.0060\n",
      "Epoch [9260/10000], Loss: 0.0059\n",
      "Epoch [9270/10000], Loss: 0.0059\n",
      "Epoch [9280/10000], Loss: 0.0059\n",
      "Epoch [9290/10000], Loss: 0.0059\n",
      "Epoch [9300/10000], Loss: 0.0059\n",
      "Epoch [9310/10000], Loss: 0.0058\n",
      "Epoch [9320/10000], Loss: 0.0058\n",
      "Epoch [9330/10000], Loss: 0.0058\n",
      "Epoch [9340/10000], Loss: 0.0058\n",
      "Epoch [9350/10000], Loss: 0.0058\n",
      "Epoch [9360/10000], Loss: 0.0057\n",
      "Epoch [9370/10000], Loss: 0.0057\n",
      "Epoch [9380/10000], Loss: 0.0057\n",
      "Epoch [9390/10000], Loss: 0.0057\n",
      "Epoch [9400/10000], Loss: 0.0057\n",
      "Epoch [9410/10000], Loss: 0.0056\n",
      "Epoch [9420/10000], Loss: 0.0056\n",
      "Epoch [9430/10000], Loss: 0.0056\n",
      "Epoch [9440/10000], Loss: 0.0056\n",
      "Epoch [9450/10000], Loss: 0.0056\n",
      "Epoch [9460/10000], Loss: 0.0056\n",
      "Epoch [9470/10000], Loss: 0.0055\n",
      "Epoch [9480/10000], Loss: 0.0055\n",
      "Epoch [9490/10000], Loss: 0.0055\n",
      "Epoch [9500/10000], Loss: 0.0055\n",
      "Epoch [9510/10000], Loss: 0.0055\n",
      "Epoch [9520/10000], Loss: 0.0054\n",
      "Epoch [9530/10000], Loss: 0.0054\n",
      "Epoch [9540/10000], Loss: 0.0054\n",
      "Epoch [9550/10000], Loss: 0.0054\n",
      "Epoch [9560/10000], Loss: 0.0054\n",
      "Epoch [9570/10000], Loss: 0.0053\n",
      "Epoch [9580/10000], Loss: 0.0053\n",
      "Epoch [9590/10000], Loss: 0.0053\n",
      "Epoch [9600/10000], Loss: 0.0053\n",
      "Epoch [9610/10000], Loss: 0.0053\n",
      "Epoch [9620/10000], Loss: 0.0052\n",
      "Epoch [9630/10000], Loss: 0.0052\n",
      "Epoch [9640/10000], Loss: 0.0052\n",
      "Epoch [9650/10000], Loss: 0.0052\n",
      "Epoch [9660/10000], Loss: 0.0052\n",
      "Epoch [9670/10000], Loss: 0.0052\n",
      "Epoch [9680/10000], Loss: 0.0051\n",
      "Epoch [9690/10000], Loss: 0.0051\n",
      "Epoch [9700/10000], Loss: 0.0051\n",
      "Epoch [9710/10000], Loss: 0.0051\n",
      "Epoch [9720/10000], Loss: 0.0051\n",
      "Epoch [9730/10000], Loss: 0.0050\n",
      "Epoch [9740/10000], Loss: 0.0050\n",
      "Epoch [9750/10000], Loss: 0.0050\n",
      "Epoch [9760/10000], Loss: 0.0050\n",
      "Epoch [9770/10000], Loss: 0.0050\n",
      "Epoch [9780/10000], Loss: 0.0050\n",
      "Epoch [9790/10000], Loss: 0.0049\n",
      "Epoch [9800/10000], Loss: 0.0049\n",
      "Epoch [9810/10000], Loss: 0.0049\n",
      "Epoch [9820/10000], Loss: 0.0049\n",
      "Epoch [9830/10000], Loss: 0.0046\n",
      "Epoch [9840/10000], Loss: 0.0045\n",
      "Epoch [9850/10000], Loss: 0.0044\n",
      "Epoch [9860/10000], Loss: 0.0044\n",
      "Epoch [9870/10000], Loss: 0.0044\n",
      "Epoch [9880/10000], Loss: 0.0043\n",
      "Epoch [9890/10000], Loss: 0.0043\n",
      "Epoch [9900/10000], Loss: 0.0043\n",
      "Epoch [9910/10000], Loss: 0.0043\n",
      "Epoch [9920/10000], Loss: 0.0043\n",
      "Epoch [9930/10000], Loss: 0.0042\n",
      "Epoch [9940/10000], Loss: 0.0042\n",
      "Epoch [9950/10000], Loss: 0.0042\n",
      "Epoch [9960/10000], Loss: 0.0042\n",
      "Epoch [9970/10000], Loss: 0.0041\n",
      "Epoch [9980/10000], Loss: 0.0041\n",
      "Epoch [9990/10000], Loss: 0.0041\n",
      "Epoch [10000/10000], Loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 99.78%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "\n",
    "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColorClassifier(\n",
       "  (fc1): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=23, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'color_classifier.pth')\n",
    "\n",
    "# Load the model for future use\n",
    "model = ColorClassifier(input_size, hidden_size, num_classes)\n",
    "model.load_state_dict(torch.load('color_classifier.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple predicitive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def hex_to_rgb(hex_color):\n",
    "    \"\"\"Convert a hex color to an RGB tuple.\"\"\"\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "# Define test colors (you can add more as needed)\n",
    "test_colors = ['#FF8488', '#FF0000', '#8B0000', '#FFFDD0']  \n",
    "test_rgb = [hex_to_rgb(color) for color in test_colors]\n",
    "\n",
    "# Convert to torch tensor\n",
    "test_rgb_tensor = torch.tensor(test_rgb, dtype=torch.float32)\n",
    "\n",
    "# Predict the categories for the test colors\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_rgb_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Define the label encoder (you should load this if you saved it)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit([\n",
    "    'Black', 'White', 'Dark-Gray', 'Gray', 'Light-Gray', 'Dark-Blue', 'Blue', 'Light-Blue',\n",
    "    'Dark-Brown', 'Brown', 'Cream', 'Dark-Red', 'Red', 'Light-Red', 'Pink', 'Purple',\n",
    "    'Dark-Green', 'Green', 'Light-Green', 'Yellow', 'Orange', 'Peach', 'Gold'\n",
    "])  # Ensure this list matches your dataset's categories\n",
    "\n",
    "# Map the predicted labels to category names\n",
    "predicted_categories = label_encoder.inverse_transform(predicted.numpy())\n",
    "\n",
    "# Plotting the results using matplotlib\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Create a color bar\n",
    "for i, (color, category) in enumerate(zip(test_colors, predicted_categories)):\n",
    "    # Plot a rectangle with the test color\n",
    "    ax.add_patch(plt.Rectangle((i*2, 0), 2, 1, color=color))\n",
    "    ax.text(i*2 + 1, 0.5, f'{color}\\n{category}', color='black', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Adjust plot limits and remove axes\n",
    "ax.set_xlim(0, len(test_colors) * 2)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
